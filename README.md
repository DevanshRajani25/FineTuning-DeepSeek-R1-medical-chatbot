# ğŸ§  Fine-Tuning DeepSeek-R1 on Medical CoT ğŸ§¬ <br/>
This project fine-tunes the DeepSeek-R1-Distill-Llama-8B model using Unsloth and LoRA on a medical reasoning dataset. The goal is to enhance the model's ability to provide expert-level answers to complex clinical questions by leveraging chain-of-thought (CoT) reasoning.

## ğŸ”§ What We Did: <br/>

ğŸ¤– Used DeepSeek-R1-Distill-Llama-8B as base LLM<br/>
ğŸš€ Integrated with Unsloth for fast + memory-efficient training<br/>
ğŸ’‰ Trained on medical Chain-of-Thought (CoT) dataset<br/>
ğŸ§  Custom prompts for clinical reasoning with step-by-step thinking<br/>
ğŸª„ Applied LoRA fine-tuning for efficiency<br/>
ğŸ“Š Tracked training using Weights & Biases (wandb)<br/>
âœ… Tested on real medical diagnosis-style questions<br/>

## ğŸ¯ Goal:<br/>
Make the model think like a medical expert with proper reasoning ğŸ©ºğŸ§ <br/>

## ğŸ“Œ Results:<br/>

ğŸ“ˆ Improved logical + accurate answers<br/>
ğŸ” Effective in diagnostics, symptoms analysis & treatment suggestions<br/>

