# 🧠 Fine-Tuning DeepSeek-R1 on Medical CoT 🧬 <br/>
This project fine-tunes the DeepSeek-R1-Distill-Llama-8B model using Unsloth and LoRA on a medical reasoning dataset. The goal is to enhance the model's ability to provide expert-level answers to complex clinical questions by leveraging chain-of-thought (CoT) reasoning.

## 🔧 What We Did: <br/>

🤖 Used DeepSeek-R1-Distill-Llama-8B as base LLM<br/>
🚀 Integrated with Unsloth for fast + memory-efficient training<br/>
💉 Trained on medical Chain-of-Thought (CoT) dataset<br/>
🧠 Custom prompts for clinical reasoning with step-by-step thinking<br/>
🪄 Applied LoRA fine-tuning for efficiency<br/>
📊 Tracked training using Weights & Biases (wandb)<br/>
✅ Tested on real medical diagnosis-style questions<br/>

## 🎯 Goal:<br/>
Make the model think like a medical expert with proper reasoning 🩺🧠<br/>

## 📌 Results:<br/>

📈 Improved logical + accurate answers<br/>
🔍 Effective in diagnostics, symptoms analysis & treatment suggestions<br/>

